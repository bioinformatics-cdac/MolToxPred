{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b445f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7bfad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"CPU\"\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d8fbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee03c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "## packages\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve,accuracy_score,log_loss,accuracy_score,f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import svm\n",
    "from keras.callbacks import EarlyStopping\n",
    "import sklearn\n",
    "import mlxtend\n",
    "import keras\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "#from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fec8c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df=pd.read_csv('/home/vinod/asetiya/train_test_data_10449_bonferroni.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf91e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(0,inplace=True)\n",
    "df.replace([np.inf, -np.inf], 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ca1c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "y=df['Toxicity']\n",
    "X=df.drop(['Toxicity'],axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de525c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select only the numerical columns\n",
    "numerical_columns = X.columns[(X.dtypes == 'float64') & (X.columns != 'IPC')]\n",
    "\n",
    "\n",
    "# Create a DataFrame with only the numerical columns\n",
    "numerical_X = X[numerical_columns]\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the numerical data and transform it for the train set\n",
    "scaled_numerical_X_train = scaler.fit_transform(numerical_X.loc[X_train.index])\n",
    "\n",
    "# Replace the scaled numerical data back into the original DataFrame for the train set\n",
    "X.loc[X_train.index, numerical_columns] = scaled_numerical_X_train\n",
    "\n",
    "# Transform the numerical data for the test set\n",
    "scaled_numerical_X_test = scaler.transform(numerical_X.loc[X_test.index])\n",
    "\n",
    "# Replace the scaled numerical data back into the original DataFrame for the test set\n",
    "X.loc[X_test.index, numerical_columns] = scaled_numerical_X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0f1351",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e74ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rn\n",
    "rn.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95165c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "# Initialize the base models\n",
    "model_lgb = lgb.LGBMClassifier(colsample_bytree= 0.5,\n",
    " lambda_l2= 0.8, \n",
    " learning_rate= 0.01,\n",
    " max_depth= 10,\n",
    " min_child_samples= 32,\n",
    " min_child_weight= 0.01,\n",
    " num_leaves= 40,\n",
    " subsample= 0.84,\n",
    " objective= 'binary',\n",
    " metric='auc',\n",
    " boosting_type= 'dart',\n",
    " is_unbalance= True,random_state=34,\n",
    " boost_from_average= False,n_estimators=100)\n",
    "\n",
    "\n",
    "\n",
    "model_rf = RandomForestClassifier(criterion= 'entropy',max_depth= 12,\n",
    "                                  min_samples_leaf= 11,min_samples_split= 39,n_estimators= 300,random_state=34,class_weight='balanced')  \n",
    "\n",
    "\n",
    "\n",
    "# Define the TensorFlow model\n",
    "def create_tf_model():\n",
    "    tf.random.set_seed(89)\n",
    "    model = tf.keras.Sequential([\n",
    "        \n",
    "    tf.keras.layers.Dense(1024, activation='relu', input_shape=(X_train.shape[1],), \n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(0.00803)),\n",
    "    tf.keras.layers.Dropout(0.306, seed=12),\n",
    "    tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.00803)),\n",
    "    tf.keras.layers.Dropout(0.306, seed=12),\n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.00803)),\n",
    "    tf.keras.layers.Dropout(0.306, seed=12),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create a KerasClassifier from the TensorFlow model\n",
    "model_tf = KerasClassifier(build_fn=create_tf_model, epochs=10, batch_size=32,class_weight=\"balanced\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce0d3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Initialize the meta-learner\n",
    "meta_learner = LogisticRegression(class_weight=\"balanced\")  \n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the stacking classifier with stratified k-fold CV\n",
    "stacked_model = StackingCVClassifier(classifiers=[model_lgb, model_rf, model_tf],\n",
    "                                     meta_classifier=meta_learner,\n",
    "                                     cv=StratifiedKFold(n_splits=5),\n",
    "                                     use_probas=True, random_state=34)\n",
    "\n",
    "# Train the stacking classifier on the training data\n",
    "stacked_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set\n",
    "train_predictions = stacked_model.predict(X_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = stacked_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "# Calculate metrics for the training set\n",
    "train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "train_f1 = f1_score(y_train, train_predictions)\n",
    "train_mcc = matthews_corrcoef(y_train, train_predictions)\n",
    "train_confusion_matrix = confusion_matrix(y_train, train_predictions)\n",
    "\n",
    "# Calculate metrics for the test set\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "test_f1 = f1_score(y_test, test_predictions)\n",
    "test_mcc = matthews_corrcoef(y_test, test_predictions)\n",
    "test_confusion_matrix = confusion_matrix(y_test, test_predictions)\n",
    "\n",
    "# Calculate AUROC using cross-validated predictions\n",
    "cross_val_predictions = stacked_model.predict_proba(X_train)[:, 1]\n",
    "cross_val_auroc = roc_auc_score(y_train, cross_val_predictions)\n",
    "\n",
    "# Print the cross-validated AUROC and metrics\n",
    "print(\"Cross-Validated AUROC:\", cross_val_auroc)\n",
    "\n",
    "print(\"Train Metrics:\")\n",
    "print(\"Accuracy:\", train_accuracy)\n",
    "print(\"F1 Score:\", train_f1)\n",
    "print(\"MCC:\", train_mcc)\n",
    "print(\"Confusion Matrix:\\n\", train_confusion_matrix)\n",
    "\n",
    "cross_val_predictions_test = stacked_model.predict_proba(X_test)[:, 1]\n",
    "cross_val_auroc_test = roc_auc_score(y_test, cross_val_predictions_test)\n",
    "\n",
    "print(\"Test Metrics:\")\n",
    "# Print the cross-validated AUROC and metrics\n",
    "print(\"Cross-Validated AUROC:\", cross_val_auroc_test)\n",
    "print(\"Accuracy:\", test_accuracy)\n",
    "print(\"F1 Score:\", test_f1)\n",
    "print(\"MCC:\", test_mcc)\n",
    "print(\"Confusion Matrix:\\n\", test_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9700c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "# Define the models\n",
    "models = [model_rf, model_tf, model_lgb]\n",
    "labels = ['Random Forest', 'Multi layer Perceptron', 'LightGBM']\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 5\n",
    "auc_scores_train = []\n",
    "auc_scores_val = []\n",
    "auc_scores_test = []\n",
    "\n",
    "\n",
    "# Iterate over models\n",
    "for model, lab in zip(models, labels):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, random_state=34, shuffle=True)\n",
    "    auc_scores_val = []  # Reset the list for validation AUC scores\n",
    "    auc_scores_test_model = []  # Separate list for test AUC scores\n",
    "    auc_scores_train_model = []\n",
    "    \n",
    "    # Perform stratified k-fold cross-validation\n",
    "    for train_index, val_index in skf.split(X_train, y_train):\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        # Train the model on the current fold\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # Predict probabilities on the training set using the individual model\n",
    "        train_predictions_proba = model.predict_proba(X_train_fold)[:, 1]\n",
    "        auc_train = roc_auc_score(y_train_fold, train_predictions_proba)\n",
    "        auc_scores_train_model.append(auc_train)\n",
    "        \n",
    "        # Predict probabilities on the validation fold\n",
    "        val_predictions = model.predict_proba(X_val_fold)[:, 1]\n",
    "        auc_val = roc_auc_score(y_val_fold, val_predictions)\n",
    "        auc_scores_val.append(auc_val)\n",
    "        \n",
    "    # Predict probabilities on the test set using the individual model\n",
    "    test_predictions_proba = model.predict_proba(X_test)[:, 1]\n",
    "    auc_test = roc_auc_score(y_test, test_predictions_proba)\n",
    "    auc_scores_test_model.append(auc_test)\n",
    "    \n",
    "    # Calculate average AUC accuracy across all folds\n",
    "    average_auc_train = np.mean(auc_scores_train_model)\n",
    "    average_auc_val = np.mean(auc_scores_val)\n",
    "    average_auc_test = np.mean(auc_scores_test_model)\n",
    "    print(f\"{lab} Average AUC _TRAIN: {average_auc_train:.4f}\")\n",
    "    print(f\"{lab} Average AUC _VAL: {average_auc_val:.4f}\")\n",
    "    print(f\"{lab} Average AUC _TEST: {average_auc_test:.4f}\")\n",
    "    \n",
    "    auc_scores_test.append(auc_scores_test_model)  # Append the list for each model\n",
    "    auc_scores_train.append(auc_scores_train_model)\n",
    "    auc_scores_val.append(auc_scores_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ff724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_scores_test\n",
    "labels=['Random Forest', 'Multi layer Perceptron', 'LightGBM']\n",
    "auc_scores = {'Model': labels, 'AUC Train': auc_scores_train, 'AUC Test': auc_scores_test,}\n",
    "df_auc = pd.DataFrame(auc_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32fa232",
   "metadata": {},
   "source": [
    "## McNemar Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712bfe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Null hypotheis: there is no significant difference between performance of the two algorithm\n",
    "#Alternate hypothesis, which argues that there is a significant difference in performance between the two algorithms\n",
    "\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "from mlxtend.evaluate import mcnemar\n",
    "\n",
    "# Assuming y_test and y_pred are available for each model\n",
    "# Replace 'model_rf', 'model_tf', 'model_lgb', and 'stacked_model' with the actual variable names\n",
    "models = [model_rf, model_tf, model_lgb, stacked_model]\n",
    "labels = ['Random Forest', 'Multi-layer Perceptron', 'LightGBM', 'Stacked']\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "results = []\n",
    "\n",
    "y_pred_stacked=stacked_model.predict(X_test)\n",
    "\n",
    "# Iterate over each model\n",
    "for model_index, model in enumerate(models):\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Create the contingency table for McNemar's test\n",
    "    table = mcnemar_table(y_target=y_test, y_model1=y_pred_stacked, y_model2=y_pred)\n",
    "    \n",
    "    # Print the table for reference\n",
    "    print(f\"\\nContingency table for {labels[model_index]} vs Stacked Model:\")\n",
    "    print(table)\n",
    "    \n",
    "    # Perform McNemar's test\n",
    "    chi2, p = mcnemar(ary=table, corrected=True)\n",
    "\n",
    "    # Store the results\n",
    "    results.append((labels[model_index], chi2, p))\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nMcNemar's Test Results:\")\n",
    "print(\"Model vs Stacked Model  Chi-squared  P-value\")\n",
    "for result in results:\n",
    "    print(f\"{result[0]:<24} {result[1]:<12.4f} {result[2]:<8.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7c49f0",
   "metadata": {},
   "source": [
    "## external validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33fbba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample=pd.read_csv(\"/home/vinod/asetiya/RSC_rev/extval_180.csv\",index_col=0)\n",
    "df_sample['Toxicity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d2bdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample=df_sample[df.columns]  #extracting relevant columns\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d14bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_sample=df_sample.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ca6298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X_sample\n",
    "X_sample = df_sample[numerical_columns]\n",
    "X_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af1d024",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scale the numerical data in df_sample using the trained scaler\n",
    "scaled_numerical_X_sample = scaler.transform(X_sample)\n",
    "\n",
    "# Create a new DataFrame with the scaled numerical data\n",
    "df_sample_scaled = pd.DataFrame(scaled_numerical_X_sample, columns=numerical_columns, index=X_sample.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08ff176",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample[numerical_columns] = df_sample_scaled\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18d8706",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample=df_sample.reindex(labels=df.columns,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4067e382",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sample=df_sample.drop('Toxicity',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab1ea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ext = stacked_model.predict(x_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14eb387",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sample=df_sample['Toxicity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0581d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ext = stacked_model.predict(x_sample)\n",
    "# Calculate accuracy\n",
    "accuracy_ext = accuracy_score(y_sample, predictions_ext)\n",
    "print(f\"accuracy_ext:{accuracy_ext}\")\n",
    "\n",
    "# Calculate F1 score\n",
    "f1_ext = f1_score(y_sample, predictions_ext)\n",
    "print(f\"f1_ext:{f1_ext}\")\n",
    "# Calculate precision\n",
    "precision_ext = precision_score(y_sample, predictions_ext)\n",
    "print(f\"precision_ext: {precision_ext}\")\n",
    "\n",
    "# Calculate recall\n",
    "recall_ext = recall_score(y_sample,predictions_ext)\n",
    "print(f\"recall_ext: {recall_ext}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a9619a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba_ext = stacked_model.predict_proba(x_sample)[:, 1]\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc_ext = roc_auc_score(y_sample, pred_proba_ext)\n",
    "print(f\"roc_auc_ext: {roc_auc_ext}\")\n",
    "\n",
    "# Confusion matrix\n",
    "ext_confusion_matrix = confusion_matrix(y_sample,predictions_ext )\n",
    "print(\"ext Confusion Matrix:\")\n",
    "print(ext_confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69303095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Prepare your new dataset (X_sample, y_sample)\n",
    "\n",
    "# Preprocess the new dataset\n",
    "\n",
    "# Predict probabilities for the new dataset\n",
    "y_pred_stacked = stacked_model.predict_proba(x_sample)[:, 1]\n",
    "\n",
    "y_pred_rf = model_rf.predict_proba(x_sample)[:, 1]\n",
    "y_pred_mlp = model_tf.predict_proba(x_sample)[:, 1]\n",
    "y_pred_lgb = model_lgb.predict_proba(x_sample)[:, 1]\n",
    "\n",
    "# Calculate AUC scores\n",
    "auc_stacked = roc_auc_score(y_sample, y_pred_stacked)\n",
    "\n",
    "auc_rf = roc_auc_score(y_sample, y_pred_rf)\n",
    "auc_mlp = roc_auc_score(y_sample, y_pred_mlp)\n",
    "auc_lgb = roc_auc_score(y_sample, y_pred_lgb)\n",
    "\n",
    "# Print the AUC scores\n",
    "print(\"Stacked Model AUC:\", auc_stacked)\n",
    "print(\"Random Forest AUC:\", auc_rf)\n",
    "print(\"Multi layer Perceptron AUC:\", auc_mlp)\n",
    "print(\"LightGBM AUC:\", auc_lgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e652dd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calibration plots\n",
    "\n",
    "\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "models = [model_rf, model_tf, model_lgb, stacked_model]\n",
    "labels = ['Random Forest', 'Multi-layer Perceptron', 'LightGBM', 'Stacked']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for model, label in zip(models, labels):\n",
    "    y_pred_ext = model.predict_proba(x_sample)[:, 1]\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(y_sample, y_pred_ext, n_bins=10)\n",
    "    plt.plot(mean_predicted_value, fraction_of_positives, \"s-\", label=label)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"Perfectly calibrated\")\n",
    "plt.xlabel(\"Mean Predicted Value\")\n",
    "plt.ylabel(\"Fraction of Positives\")\n",
    "plt.title(\"Calibration Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.savefig('/home/md/asetiya/new_padel_fingerprints_10449/Calibration_ext.png')\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4420f7",
   "metadata": {},
   "source": [
    "## etoxpred_probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcdc8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the file from \"etoxpred\" containing molecule SMILES and toxicity probability scores\n",
    "etoxpred_data = pd.read_csv(\"/home/vinod/asetiya/RSC_rev/share/results_smiles_test_etoxpred.csv\")  # Replace 'etoxpred_results.csv' with the actual file name\n",
    "\n",
    "# Process the file to extract the molecule SMILES and corresponding toxicity probability scores\n",
    "etoxpred_probabilities = etoxpred_data['Tox-score']\n",
    "\n",
    "# Predict toxicity probabilities using your model on the same set of molecules\n",
    "moltoxpred_probabilities = test_predictions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0385f384",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the file from \"etoxpred\" containing molecule SMILES and toxicity probability scores\n",
    "etoxpred_data_ext= pd.read_csv(\"/home/vinod/asetiya/RSC_rev/share/results_smiles_ext_etoxpred.csv\")  # Replace 'etoxpred_results.csv' with the actual file name\n",
    "\n",
    "# Process the file to extract the molecule SMILES and corresponding toxicity probability scores\n",
    "etoxpred_probabilities_ext = etoxpred_data_ext['Tox-score']\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2934aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the models\n",
    "models = [model_rf, model_tf, model_lgb]\n",
    "labels = ['Random Forest', 'Multi layer Perceptron', 'LightGBM']\n",
    "\n",
    "# Calculate the average AUC values for each model\n",
    "average_auc_values = [np.mean(auc_scores_test[i]) for i in range(len(models))]\n",
    "\n",
    "# Plot the AUC-ROC curve with average AUC values for individual models\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(len(models)):\n",
    "    fpr, tpr, _ = roc_curve(y_test, models[i].predict_proba(X_test)[:, 1])\n",
    "    roc_auc = average_auc_values[i]\n",
    "    plt.plot(fpr, tpr, label=f'{labels[i]} (AUC = {roc_auc:.4f})')\n",
    "\n",
    "# Plot the AUC-ROC curve for the stacked model\n",
    "fpr_stacked, tpr_stacked, _ = roc_curve(y_test, stacked_model.predict_proba(X_test)[:, 1])\n",
    "roc_auc_stacked = cross_val_auroc_test\n",
    "plt.plot(fpr_stacked, tpr_stacked, label=f'Stacked Model (AUC = {roc_auc_stacked:.4f})')\n",
    "\n",
    "# Plot the AUC-ROC curve for etoxpred\n",
    "fpr_etoxpred, tpr_etoxpred, _ = roc_curve(y_test, etoxpred_probabilities)\n",
    "roc_auc_etoxpred = auc(fpr_etoxpred, tpr_etoxpred)\n",
    "plt.plot(fpr_etoxpred, tpr_etoxpred, label=f'etoxpred (AUC = {roc_auc_etoxpred:.4f})', linestyle='--')\n",
    "\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xlabel('False Positive Rate',fontsize=14)\n",
    "plt.ylabel('True Positive Rate',fontsize=14)\n",
    "plt.title('Comparison of Receiver Operating Characteristic for Test Set',fontsize=14)\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Display the plot\n",
    "#plt.savefig(\"/home/vinod/asetiya/RSC_rev/plots/AUC_test_bonferroni_balanced.png\",dpi=600)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7622fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for external validation test (180)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Compute the false positive rate (FPR), true positive rate (TPR), and AUC for each model\n",
    "fpr_stacked, tpr_stacked, _ = roc_curve(y_sample, stacked_model.predict_proba(x_sample)[:, 1])\n",
    "roc_auc_stacked = auc(fpr_stacked, tpr_stacked)\n",
    "\n",
    "fpr_model_rf, tpr_model_rf, _ = roc_curve(y_sample, model_rf.predict_proba(x_sample)[:, 1])\n",
    "roc_auc_model_rf = auc(fpr_model_rf, tpr_model_rf)\n",
    "\n",
    "fpr_model_tf, tpr_model_tf, _ = roc_curve(y_sample, model_tf.predict_proba(x_sample)[:, 1])\n",
    "roc_auc_model_tf = auc(fpr_model_tf, tpr_model_tf)\n",
    "\n",
    "fpr_model_lgb, tpr_model_lgb, _ = roc_curve(y_sample, model_lgb.predict_proba(x_sample)[:, 1])\n",
    "roc_auc_model_lgb = auc(fpr_model_lgb, tpr_model_lgb)\n",
    "\n",
    "# Plot the AUC-ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.plot(fpr_model_rf, tpr_model_rf, label='Random Forest (AUC = {:.4f})'.format(roc_auc_model_rf))\n",
    "plt.plot(fpr_model_tf, tpr_model_tf, label='Multi layer Perceptron(AUC = {:.4f})'.format(roc_auc_model_tf))\n",
    "plt.plot(fpr_model_lgb, tpr_model_lgb, label='LightGBM (AUC = {:.4f})'.format(roc_auc_model_lgb))\n",
    "plt.plot(fpr_stacked, tpr_stacked, label='Stacked Model (AUC = {:.4f})'.format(roc_auc_stacked))\n",
    "\n",
    "# Plot the AUC-ROC curve for etoxpred\n",
    "fpr_etoxpred, tpr_etoxpred, _ = roc_curve(y_sample, etoxpred_probabilities_ext)\n",
    "roc_auc_etoxpred = auc(fpr_etoxpred, tpr_etoxpred)\n",
    "plt.plot(fpr_etoxpred, tpr_etoxpred, label=f'etoxpred (AUC = {roc_auc_etoxpred:.4f})', linestyle='--')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xlabel('False Positive Rate',fontsize=14)\n",
    "plt.ylabel('True Positive Rate',fontsize=14)\n",
    "plt.title('Comparison of Receiver Operating Characteristic for external validation set',fontsize=14)\n",
    "plt.legend()\n",
    "#plt.savefig(\"/home/vinod/asetiya/RSC_rev/plots/ROC_AUC_external_validation_bonferroni.png\",dpi=600)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
