{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16251e3a-d142-4e96-93e4-7533d726ec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f612810-64cd-4eb8-8461-6d9e34f9f21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635d6089-9818-4413-ab2b-e8c2c7ed1c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"/home/vinod/asetiya/train_test_data_10449_bonferroni.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511dedd5-5e4d-4433-a9f1-5a00ca1cc17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)\n",
    "df.fillna(0,inplace=True)\n",
    "df.replace([np.inf, -np.inf], 0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c68f3b2-b7b9-41b0-843f-dc2f53419341",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['Toxicity']\n",
    "X=df.drop(['Toxicity'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c26a50-ab2a-4af8-83bc-bd06cba31b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b24724f-b16c-4f64-827a-6ea803c81c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of the train and test sets\n",
    "train_indices = X_train.index\n",
    "test_indices = X_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76bbae7-0601-4b09-9956-7a829d68fe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select only the numerical columns\n",
    "numerical_columns = X.columns[(X.dtypes == 'float64') & (X.columns != 'IPC')]\n",
    "\n",
    "\n",
    "# Create a DataFrame with only the numerical columns\n",
    "numerical_X = X[numerical_columns]\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the numerical data and transform it for the train set\n",
    "scaled_numerical_X_train = scaler.fit_transform(numerical_X.loc[X_train.index])\n",
    "\n",
    "# Replace the scaled numerical data back into the original DataFrame for the train set\n",
    "X.loc[X_train.index, numerical_columns] = scaled_numerical_X_train\n",
    "\n",
    "# Transform the numerical data for the test set\n",
    "scaled_numerical_X_test = scaler.transform(numerical_X.loc[X_test.index])\n",
    "\n",
    "# Replace the scaled numerical data back into the original DataFrame for the test set\n",
    "X.loc[X_test.index, numerical_columns] = scaled_numerical_X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47034e5-c4c8-43a4-a2cc-afa80b8b2e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a64b806-6f9f-4486-b15b-a56fa0f7312e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to the appropriate data types\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09de21d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "from bayes_opt import BayesianOptimization\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "# Assuming y_train is your target variable\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "\n",
    "# Convert class weights to a dictionary\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "\n",
    "# Assuming X_train, y_train are already defined\n",
    "# You may need to load your dataset before running this code\n",
    "\n",
    "# Define the MLP architecture using TensorFlow\n",
    "def create_model(learning_rate, dropout_rate,regularization_strength):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(1024, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=tf.keras.regularizers.l2(regularization_strength)),\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(regularization_strength)),\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(regularization_strength)),\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "\n",
    "    # Compile the model with the given learning rate\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define the parameter search space\n",
    "# Define the parameter search space\n",
    "pbounds = {\n",
    "    'learning_rate': (0.001, 0.1),\n",
    "    'dropout_rate': (0.2, 0.5),\n",
    "\n",
    "    'regularization_strength': (1e-5, 0.01),  # Range for L2 regularization strength\n",
    "}\n",
    "\n",
    "\n",
    "# Define the function to optimize with cross-validation\n",
    "def optimize_model_cv(learning_rate, dropout_rate,  regularization_strength):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    val_accuracies = []\n",
    "\n",
    "    for train_index, val_index in skf.split(X_train, y_train):\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "        model = create_model(learning_rate, dropout_rate,  regularization_strength)\n",
    "        history = model.fit(X_train_fold, y_train_fold, epochs=10, batch_size=32, verbose=0,\n",
    "                            class_weight=class_weight_dict, validation_data=(X_val_fold, y_val_fold))\n",
    "        val_accuracies.append(history.history['val_accuracy'][-1])\n",
    "\n",
    "    average_val_accuracy = np.mean(val_accuracies)\n",
    "    return average_val_accuracy\n",
    "\n",
    "\n",
    "# Perform Bayesian optimization\n",
    "optimizer = BayesianOptimization(f=optimize_model_cv, pbounds=pbounds)\n",
    "optimizer.maximize(init_points=5, n_iter=5)\n",
    "\n",
    "# Print the best hyperparameters and corresponding accuracy\n",
    "print(\"Best Hyperparameters: \", optimizer.max['params'])\n",
    "print(\"Best Average Validation Accuracy: \", optimizer.max['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0c5944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best hyperparameters and corresponding accuracy\n",
    "print(\"Best Hyperparameters: \", optimizer.max['params'])\n",
    "print(\"Best Average Validation Accuracy: \", optimizer.max['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2d1f57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
